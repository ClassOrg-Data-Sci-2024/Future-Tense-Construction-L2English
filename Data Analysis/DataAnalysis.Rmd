---
title: "FutureTenseConstructionL2English"
author: "Daniel Crawford"
date: "04/02/2024"
output: 
  github_document: 
    toc: TRUE
---

```{r}
#Import Packages
library(tidyverse)
```



# Pull in final data

## Pull in Final Data from Data Processing
```{r}
if (file.exists(paste0(dirname(getwd()),'/Data Files/FINAL_DATA_countruction_counts_and_student_info_with_scores.csv'))){
  print('Reading in File from directory')
  construction_counts = as_tibble(read.csv(paste0(dirname(getwd()),'/Data Files/FINAL_DATA_countruction_counts_and_student_info_with_scores.csv')))
}else{
 print('The final data dile you are looking for (FINAL_DATA_countruction_counts_and_student_info_with_scores.csv) does not appear to be in a directory.') 
}
```

## Pull in Student Info 
```{r}
student_info = as_tibble(read.csv(url("https://github.com/ELI-Data-Mining-Group/PELIC-dataset/raw/master/corpus_files/student_information.csv")))
```

## Pull in PELIC Compiled
```{r}
if (file.exists(paste0(dirname(getwd()),'/large_data/pelic_compiled.csv'))){
  print('Reading in File from directory')
  PELIC_compiled = read.csv(paste0(dirname(getwd()),'/large_data/pelic_compiled.csv'))
}else{
  print('Reading in File from url')
  PELIC_compiled = as_tibble(read.csv(url("https://github.com/ELI-Data-Mining-Group/PELIC-dataset/raw/master/PELIC_compiled.csv"), fileEncoding = "ISO-8859-1"))
}

```


# Summary Statistics

```{r}
#Counts of Each Construction
construction_counts %>% 
  select(count_will_construction, count_goingTo_construction) %>% 
  summarise_all(sum)
```



```{r}
#Get proportions
construction_props = construction_counts %>% 
  mutate(prop_goingTo_construction = count_goingTo_construction/(count_will_construction+count_goingTo_construction), .after = anon_id) %>% 
  mutate(prop_will_construction = count_will_construction/(count_will_construction+count_goingTo_construction), .after = anon_id)

construction_props

```

## Distributions of Scores
```{r}
#Visualize Raw Distribution of Scores

construction_props %>% 
  #only use scores
  select(LCT_Score, MTELP_I, MTELP_II, MTELP_III, MTELP_Conv_Score, Writing_Sample) %>% 
  pivot_longer(everything()) %>% 
  ggplot(aes(x = value)) +
  geom_histogram() +
  facet_wrap(~name)+
  labs(title = "Proficiency Score Distributions", x = "Value (Raw)", y = "Frequency") +
  theme_minimal()



construction_props %>% 
  #only check writing samples
  select(Writing_Sample) %>% 
  pivot_longer(everything()) %>% 
  ggplot(aes(x = value)) +
  geom_histogram() +
  facet_wrap(~name)+
  labs(title = "Proficiency Score Distributions", x = "Value (Raw)", y = "Frequency") +
  theme_minimal()


```





## Scale Proficiency Scores

```{r}
#Scale Scores

scaled_construction_props = construction_props %>% 
  select(anon_id, prop_will_construction, prop_goingTo_construction, LCT_Score, MTELP_I, MTELP_II, MTELP_III, MTELP_Conv_Score, Writing_Sample) %>% 
  mutate(across(-c(anon_id,prop_will_construction, prop_goingTo_construction), ~ scale(.x))) 
  
  
scaled_construction_props
```


```{r}

#Get Scaled values, but only for students who used future tense at least 30 times. 

scaled_construction_props_30plus = construction_counts %>% 
  filter(count_will_construction+count_goingTo_construction >= 30) %>% 
  mutate(prop_goingTo_construction = count_goingTo_construction/(count_will_construction+count_goingTo_construction), .after = anon_id) %>% 
  mutate(prop_will_construction = count_will_construction/(count_will_construction+count_goingTo_construction), .after = anon_id) %>% 
  select(anon_id, prop_will_construction, prop_goingTo_construction, LCT_Score, MTELP_I, MTELP_II, MTELP_III, MTELP_Conv_Score, Writing_Sample) %>% 
  mutate(across(-c(anon_id,prop_will_construction, prop_goingTo_construction), ~ scale(.x)))
  

scaled_construction_props_30plus
```








# Correlation Analysis

```{r}
scaled_construction_props_30plus %>% 
  select(prop_will_construction, LCT_Score, MTELP_I, MTELP_II, MTELP_III, MTELP_Conv_Score, Writing_Sample) %>% 
  pivot_longer(-prop_will_construction) %>% 
  ggplot(aes(prop_will_construction, value))+
  geom_point()+
  facet_wrap(~name)+
  labs(title = "Proficiency Score Distributions (more than 30 uses of future tense)", x = "Proportion of 'will' construction", y = "Proficinecy Test Score")
```




```{r}
create_corr_diagram = function(scaled_construction_props_30plus, title_str){
  
  cm = as_tibble(cor(scaled_construction_props_30plus %>%  
           filter(!is.na(prop_will_construction)) %>%
           select(prop_will_construction, LCT_Score, MTELP_I, MTELP_II, MTELP_III, MTELP_Conv_Score, Writing_Sample)))
  
  

  cm %>% 
    mutate(score = colnames(cm)) %>%
    pivot_longer(!score) %>%
    mutate(value = round(value,2)) %>% 
    
    ggplot(aes(x = score, y = name, fill = value)) +
    
    geom_tile(color = "white") +
    scale_fill_gradient2(
      low = "blue",
      high = "red",
      mid = "white",
      midpoint = 0,
      limit = c(-1, 1),
      space = "Lab",
      name = "Pearson\nCorrelation"
    ) +
    theme_minimal() + # minimal theme
    theme(axis.text.x = element_text(
      angle = 45,
      vjust = 1,
      size = 12,
      hjust = 1
    )) +
    coord_fixed() +
    geom_text(aes(score, name, label = value),
              color = "black",
              size = 4) +
    theme(
      axis.title.x = element_blank(),
      axis.title.y = element_blank(),
      panel.grid.major = element_blank(),
      panel.border = element_blank(),
      panel.background = element_blank(),
      axis.ticks = element_blank(),
      legend.justification = c(1, 0),
      legend.direction = "horizontal"
    ) +
    guides(fill = guide_colorbar(
      barwidth = 7,
      barheight = 1,
      title.position = "top",
      title.hjust = 0.5
    ))+
    labs(title = paste0("Scaled Proficiency Scores Correlation (30 plus) - ",title_str))
    
}
```



```{r}
create_corr_diagram(scaled_construction_props_30plus, "All Langs")
```





```{r}
construction_props %>%
  #Get Only languages with at least 30 L1 speakers
  group_by(native_language) %>% 
  filter(n()>30) %>% 
  select(native_language, prop_will_construction, prop_goingTo_construction) %>% 
  
  summarise(across(everything(), .f = list(mean = mean), na.rm = TRUE)) %>% 
  pivot_longer(cols = starts_with('prop')) %>% 
  
  
  #Plotting
  mutate(name = str_split_i(name, "_",2)) %>% 
  arrange(desc(value)) %>% 
  mutate(Group = factor(native_language, levels = unique(native_language))) %>% 
  ggplot(aes(Group, value, fill = name))+
    geom_bar(stat = 'identity', position = 'dodge')+
    labs(title = "L1 In Future Tense Construction Proportion", x = "Native Language (L1)", y = "Proportion", fill = "Construction") +
    theme_minimal()
  
```


```{r}
scaled_construction_props_30plus_withL1 = scaled_construction_props_30plus %>% 
  left_join(construction_counts %>% select(anon_id, native_language), by = 'anon_id') %>% 
  group_by(native_language) %>% 
  filter(n()>10)

scaled_construction_props_30plus_withL1 %>% count(native_language)
```


## Correlations For each L1

```{r}
create_corr_diagram(
  scaled_construction_props_30plus_withL1 %>% filter(native_language == 'Arabic') %>% ungroup() %>%  select(-native_language,-anon_id), "Arabic")

create_corr_diagram(
  scaled_construction_props_30plus_withL1 %>% filter(native_language == 'Chinese') %>% ungroup() %>%  select(-native_language,-anon_id), "Chinese")

create_corr_diagram(
  scaled_construction_props_30plus_withL1 %>% filter(native_language == 'Japanese') %>% ungroup() %>%  select(-native_language,-anon_id), "Japanese")

create_corr_diagram(
  scaled_construction_props_30plus_withL1 %>% filter(native_language == 'Korean') %>% ungroup() %>%  select(-native_language,-anon_id), "Korean")

create_corr_diagram(
  scaled_construction_props_30plus_withL1 %>% filter(native_language == 'Thai') %>% ungroup() %>%  select(-native_language,-anon_id), "Thai")


```
# Random Intercepts Analysis

## Token Level Analysis

```{r}
if (file.exists(paste0(dirname(getwd()),'/large_data/tokenized_df_unnested.csv'))){
  print('Reading in File from directory')
  tokenized_df_unnested = read.csv(paste0(dirname(getwd()),'/large_data/tokenized_df_unnested.csv'))
}else{
  
  print('Creating tokenized_df_unnested - about 10 minute runtime.')
  # !!! WARNING - this will take about 10 - 15 !!!
  
  #Create tokenized_df, each row is a token
  tokenized_df_unnested = PELIC_compiled %>%
   #create tokenized df from the tok_lem_POS string
   mutate(tokenized_nested_df = map(tok_lem_POS, tlp_text_to_df)) %>%
   #keep the unique identifier of answer id
   select(answer_id, tokenized_nested_df) %>%
   #make the df longer and unndest
   unnest(tokenized_nested_df)

  # un-comment if you want to save
  #tokenized_df_unnested %>% write.csv('large_data/tokenized_df_unnested.csv')
}
```

```{r}
#Join in information for each token
all_data = tokenized_df_unnested %>% 
  left_join(PELIC_compiled, by = 'answer_id') %>% 
  left_join(student_info, by = 'anon_id') %>% 
  left_join(scaled_construction_props, by = 'anon_id')
```




```{r}
#Using the smae filtes as in data processing, get only the future tokens
future_tokens_data = all_data %>% 
  mutate(is_will_construction = if_else(trimws(lemma) == 'will' & trimws(POS) == 'MD', 1,0)) %>% 
  #if there is a 'going' token followed by 'to' then a verb within 2 words, put 1 in new column
  mutate(is_goingTo_construction = 
            if_else(
              token == "going" & lead(token) == "to" & (
                #lead allows to 'look ahead'
                startsWith(trimws(lead(POS, 2)), 'V') | 
                startsWith(trimws(lead(POS, 3)), 'V')
              ),1,0)) %>% 
  filter(is_goingTo_construction == 1 | is_will_construction == 1) %>% 
  mutate(mean_prof_score = rowMeans(select(.,LCT_Score, MTELP_Conv_Score, Writing_Sample), na.rm = T)) 

future_tokens_data
```


### Predict 'will' with Mean of Prof Scores using Random Intercepts

```{r}
mean_prof_score.intercepts_model <- lmer(is_will_construction ~ (1 | anon_id) + mean_prof_score, data = future_tokens_data)

mean_prof_score.intercepts_model.coefs = coef(mean_prof_score.intercepts_model)$anon_id %>% 
  #Change names to interpres
  rename(Intercept = `(Intercept)`, Slope = mean_prof_score) %>% 
  rownames_to_column("anon_id")

mean_prof_score.intercepts_model.coefs

```
#### Visualize by Native Languages

```{r}
mean_prof_score.intercepts_model.coefs %>% 
  #Bring in student info
  left_join(student_info %>% select(anon_id, native_language), by = 'anon_id') %>% 
  #Only use L1s with at least 30 students
  group_by(native_language) %>% filter(n()>30) %>% 
  left_join(future_tokens_data %>% select(anon_id, mean_prof_score), by = 'anon_id') %>% 
  ggplot()+
  geom_point(aes(x = mean_prof_score, y = Intercept)) +
  facet_wrap(~ native_language)

mean_prof_score.intercepts_model.coefs %>% 
  #Bring in student info
  left_join(student_info %>% select(anon_id, native_language), by = 'anon_id') %>% 
   #Only use L1s with at least 30 students
  group_by(native_language) %>% filter(n()>30) %>% 
  left_join(future_tokens_data %>% select(anon_id, mean_prof_score), by = 'anon_id') %>% 
  ggplot()+
  geom_boxplot(aes(x = native_language, y = Intercept))
```


### Predict 'will' with Mean of Prof Scores using Random Intercepts and Random Slopes

```{r}
mean_prof_score.random_slope_and_intercepts_model <- lmer(is_will_construction ~ mean_prof_score + (1 + mean_prof_score | anon_id), data = future_tokens_data)

summary(mean_prof_score.random_slope_and_intercepts_model)

mean_prof_score.random_slope_and_intercepts_model.coefs = coef(mean_prof_score.random_slope_and_intercepts_model)$anon_id %>% 
  rename(Intercept = `(Intercept)`) %>% 
  rownames_to_column("anon_id")



mean_prof_score.random_slope_and_intercepts_model.coefs %>% 
  left_join(future_tokens_data %>% select(anon_id, level_id, native_language) %>% mutate(level_id = factor(level_id)), by = 'anon_id') %>%
  distinct() %>%
  ggplot()+
  geom_point(aes(Intercept, mean_prof_score, color = level_id))+
  facet_wrap(~level_id)+
  labs(title = 'Intercept and Coefficient of Mean Proficency Score by Level')
```



```{r}

#ANOVA for Intercepts
print('ANOVA for Intercepts by Level')
summary(aov(Intercept ~ level_id, data = mean_prof_score.random_slope_and_intercepts_model.coefs %>% 
  left_join(future_tokens_data %>% select(anon_id, level_id, native_language) %>% mutate(level_id = factor(level_id)), by = 'anon_id') %>%
  distinct()))

print('ANOVA for Slopes by Level')
summary(aov(mean_prof_score ~ level_id, data = mean_prof_score.random_slope_and_intercepts_model.coefs %>% 
  left_join(future_tokens_data %>% select(anon_id, level_id, native_language) %>% mutate(level_id = factor(level_id)), by = 'anon_id') %>%
  distinct()))


```
### Predict 'will' with Years of English Learning using Random Intercepts

```{r}
yrs_learning_eng_intercepts_model <- lmer(is_will_construction ~ (1 | anon_id) + yrs_of_english_learning , data = future_tokens_data)

yrs_learning_eng_model_coefs = coef(yrs_learning_eng_intercepts_model)$anon_id %>% 
  rename(Intercept = `(Intercept)`) %>% 
  rownames_to_column("anon_id")

yrs_learning_eng_model_coefs %>% 
  left_join(student_info %>% select(anon_id, yrs_of_english_learning, native_language), by = 'anon_id') %>% 
  group_by(native_language) %>% filter(n()>30) %>% 
  ggplot()+
  geom_boxplot(aes(x = yrs_of_english_learning, y = Intercept))
```
### Predict 'will' with Level using Random Intercepts

```{r}
level_id.intercepts_model <- lmer(is_will_construction ~ (1 | anon_id) + level_id, data = future_tokens_data %>% mutate(level_id = factor(level_id)))

level_id.intercepts_model

level_id.model_coefs = coef(level_id.intercepts_model)$anon_id %>% 
  rename(Intercept = `(Intercept)`) %>% 
  rownames_to_column("anon_id")

level_id.model_coefs %>% 
  left_join(student_info %>% select(anon_id, native_language), by = 'anon_id') %>% 
  left_join(future_tokens_data %>% select(anon_id, level_id), by = 'anon_id') %>% 
  ggplot()+
  geom_boxplot(aes(x = factor(level_id), y = Intercept)) + 
  labs(x = "ELI Level", y = "Random Speaker Intercept", title = "Effects of ELI Level on Speaker Intercept")
```
#### ANOVA of Prediction 'will' with Level Id using Random Intercepts
```{r}
summary(aov(Intercept ~ level_id, data = level_id.model_coefs %>% 
  select(anon_id,Intercept) %>% 
  left_join(future_tokens_data %>% select(anon_id, level_id), by = 'anon_id') %>% 
  mutate(level_id = factor(level_id)) %>% 
  distinct()))
```




```{r}
text_len.intercepts_model <- lmer(is_will_construction ~ text_len + (1 + text_len | anon_id), data = future_tokens_data %>% mutate(text_len = scale(text_len)))

text_len.intercepts_model

text_len.model_coefs = coef(text_len.intercepts_model)$anon_id %>% 
  rename(Intercept = `(Intercept)`) %>% 
  rownames_to_column("anon_id")

text_len.model_coefs %>% 
  left_join(student_info %>% select(anon_id, native_language), by = 'anon_id') %>% 
  left_join(future_tokens_data %>% select(anon_id, text_len, level_id), by = 'anon_id') %>% 
  ggplot()+
  geom_point(aes(x = text_len.y, y = Intercept))+
  facet_wrap(~level_id)


```





## Token Context Analysis

```{r}
future_tokens_data
```

```{r}
tokenized_df_unnested %>% 
  mutate(will_next_V_code = if_else(
    trimws(lemma) == 'will' & trimws(POS) == 'MD', 
    if_else(
      startsWith(trimws(lead(POS)),'V'),
      lead(POS),
      if_else(
        startsWith(trimws(lead(POS,2)),'V'),
        lead(POS,2),
        lead(POS,3)
      )
    ),
    'NotV')) %>% 
 
  filter(trimws(lemma) == 'will' & trimws(POS) == 'MD') %>% 
  mutate(will_next_V_code = if_else(
    startsWith(trimws(will_next_V_code),'V'), will_next_V_code,'Other_NonV' 
  )) %>% 
  count(will_next_V_code)
```

```{r}
tokenized_df_unnested %>% 
  mutate(goingTo_next_V_code = if_else(
      token == "going" & lead(token) == "to" & (
        #lead allows to 'look ahead'
        startsWith(trimws(lead(POS, 2)), 'V') | 
        startsWith(trimws(lead(POS, 3)), 'V')),
      
    if_else(
      startsWith(trimws(lead(POS)),'V'),
      lead(POS),
      if_else(
        startsWith(trimws(lead(POS,2)),'V'),
        lead(POS,2),
        lead(POS,3)
      )
    ),
    'NotV')) %>% 
 
  filter(token == "going" & lead(token) == "to") %>% 
  count(goingTo_next_V_code)
```

