---
title: "FutureTenseConstructionL2English"
author: "Daniel Crawford"
date: "04/24/2024"
output: 
  github_document: 
    toc: TRUE
---

```{r, setup}
knitr::opts_chunk$set(fig.path = paste0(dirname(getwd()),'/images/DataAnalysis-'))
```


```{r, label = 'import'}
#Import Packages
library(tidyverse)
library(lme4)
```



# Pull in final data

## Pull in Final Data from Data Processing
```{r, label = 'readFinalData'}
if (file.exists(paste0(dirname(getwd()),'/Data Files/FINAL_DATA_countruction_counts_and_student_info_with_scores.csv'))){
  print('Reading in File from directory')
  construction_counts = as_tibble(read.csv(paste0(dirname(getwd()),'/Data Files/FINAL_DATA_countruction_counts_and_student_info_with_scores.csv')))
}else{
 print('The final data dile you are looking for (FINAL_DATA_countruction_counts_and_student_info_with_scores.csv) does not appear to be in a directory.') 
}
```

## Pull in Student Info 
```{r, label = 'readStudentInfo'}
student_info = as_tibble(read.csv(url("https://github.com/ELI-Data-Mining-Group/PELIC-dataset/raw/master/corpus_files/student_information.csv")))
```

## Pull in PELIC Compiled
```{r, label = 'readPelicCompiled'}
if (file.exists(paste0(dirname(getwd()),'/large_data/pelic_compiled.csv'))){
  print('Reading in File from directory')
  PELIC_compiled = read.csv(paste0(dirname(getwd()),'/large_data/pelic_compiled.csv'))
}else{
  print('Reading in File from url')
  PELIC_compiled = as_tibble(read.csv(url("https://github.com/ELI-Data-Mining-Group/PELIC-dataset/raw/master/PELIC_compiled.csv"), fileEncoding = "ISO-8859-1"))
}

```


# Summary Statistics

```{r, label = 'countOfConstructions'}
#Counts of Each Construction
construction_counts %>% 
  select(count_will_construction, count_goingTo_construction) %>% 
  summarise_all(sum)
```




```{r, label = 'getProportions'}
#Get proportions
construction_props = construction_counts %>% 
  mutate(prop_goingTo_construction = count_goingTo_construction/(count_will_construction+count_goingTo_construction), .after = anon_id) %>% 
  mutate(prop_will_construction = count_will_construction/(count_will_construction+count_goingTo_construction), .after = anon_id)

construction_props

```

## Distributions of Scores
```{r, label = 'distScores'}
#Visualize Raw Distribution of Scores

construction_props %>% 
  #only use scores
  select(LCT_Score, MTELP_I, MTELP_II, MTELP_III, MTELP_Conv_Score, Writing_Sample) %>% 
  pivot_longer(everything()) %>% 
  ggplot(aes(x = value)) +
  geom_histogram() +
  facet_wrap(~name)+
  labs(title = "Proficiency Score Distributions", x = "Value (Raw)", y = "Frequency") +
  theme_minimal()
```



```{r, label = 'distWritingScores'}
construction_props %>% 
  #only check writing samples
  select(Writing_Sample) %>% 
  pivot_longer(everything()) %>% 
  ggplot(aes(x = value)) +
  geom_histogram() +
  facet_wrap(~name)+
  labs(title = "Proficiency Score Distributions", x = "Value (Raw)", y = "Frequency") +
  theme_minimal()


```





## Scale Proficiency Scores

```{r, label = 'scaleProfScores'}
#Scale Scores

scaled_construction_props = construction_props %>% 
  select(anon_id, prop_will_construction, prop_goingTo_construction, LCT_Score, MTELP_I, MTELP_II, MTELP_III, MTELP_Conv_Score, Writing_Sample) %>% 
  mutate(across(-c(anon_id,prop_will_construction, prop_goingTo_construction), ~ scale(.x))) 
  
  
scaled_construction_props
```


```{r, label = 'scaleProfScores30plus'}

#Get Scaled values, but only for students who used future tense at least 30 times. 

scaled_construction_props_30plus = construction_counts %>% 
  filter(count_will_construction+count_goingTo_construction >= 30) %>% 
  mutate(prop_goingTo_construction = count_goingTo_construction/(count_will_construction+count_goingTo_construction), .after = anon_id) %>% 
  mutate(prop_will_construction = count_will_construction/(count_will_construction+count_goingTo_construction), .after = anon_id) %>% 
  select(anon_id, prop_will_construction, prop_goingTo_construction, LCT_Score, MTELP_I, MTELP_II, MTELP_III, MTELP_Conv_Score, Writing_Sample) %>% 
  mutate(across(-c(anon_id,prop_will_construction, prop_goingTo_construction), ~ scale(.x)))
  

scaled_construction_props_30plus
```








# Correlation Analysis

```{r, label = 'corrAn'}
scaled_construction_props_30plus %>% 
  select(prop_will_construction, LCT_Score, MTELP_I, MTELP_II, MTELP_III, MTELP_Conv_Score, Writing_Sample) %>% 
  pivot_longer(-prop_will_construction) %>% 
  ggplot(aes(prop_will_construction, value))+
  geom_point()+
  facet_wrap(~name)+
  labs(title = "Proficiency Score Distributions (more than 30 uses of future tense)", x = "Proportion of 'will' construction", y = "Proficinecy Test Score")
```




# Random Intercepts Analysis

## Token Level Analysis

```{r, label = 'readTokenizedDF'}
if (file.exists(paste0(dirname(getwd()),'/large_data/tokenized_df_unnested.csv'))){
  print('Reading in File from directory')
  tokenized_df_unnested = read.csv(paste0(dirname(getwd()),'/large_data/tokenized_df_unnested.csv'))
}else{
  
  print('Creating tokenized_df_unnested - about 10 minute runtime.')
  # !!! WARNING - this will take about 10 - 15 !!!
  
  #Create tokenized_df, each row is a token
  tokenized_df_unnested = PELIC_compiled %>%
   #create tokenized df from the tok_lem_POS string
   mutate(tokenized_nested_df = map(tok_lem_POS, tlp_text_to_df)) %>%
   #keep the unique identifier of answer id
   select(answer_id, tokenized_nested_df) %>%
   #make the df longer and unnest
   unnest(tokenized_nested_df)

  # un-comment if you want to save
  #tokenized_df_unnested %>% write.csv('large_data/tokenized_df_unnested.csv')
}
```

```{r, label = 'joinDataToTokenizedDf'}
#Join in information for each token
all_data = tokenized_df_unnested %>% 
  left_join(PELIC_compiled, by = 'answer_id') %>% 
  left_join(student_info, by = 'anon_id') %>% 
  left_join(scaled_construction_props, by = 'anon_id')
```


There were some problems when knitting, so I saved `future_tokens_data.csv` in my `large_data` folder.
```{r, label = 'makeFutureTokensDF'}
if (file.exists(paste0(dirname(getwd()),'/large_data/future_tokens_data.csv'))){
  print('Reading in File from directory')
  future_tokens_data = read.csv(paste0(dirname(getwd()),'/large_data/future_tokens_data.csv'))
}else{
  
  print('Creating future_tokens_data')
  # !!! WARNING - this can have long runtime!!!
  
  future_tokens_data = all_data %>% 
  mutate(is_will_construction = if_else(trimws(lemma) == 'will' & trimws(POS) == 'MD', 1,0)) %>% 
  #if there is a 'going' token followed by 'to' then a verb within 2 words, put 1 in new column
  mutate(is_goingTo_construction = 
            if_else(
              token == "going" & lead(token) == "to" & (
                #lead allows to 'look ahead'
                startsWith(trimws(lead(POS, 2)), 'V') | 
                startsWith(trimws(lead(POS, 3)), 'V')
              ),1,0)) %>% 
  filter(is_goingTo_construction == 1 | is_will_construction == 1) %>% 
  mutate(mean_prof_score = rowMeans(select(.,LCT_Score, MTELP_Conv_Score, Writing_Sample), na.rm = T)) 

  future_tokens_data %>% write.csv(paste0(dirname(getwd()),'/large_data/future_tokens_data.csv'))

}

```






### Predict 'will' with Mean of Prof Scores using Random Intercepts and Random Slopes

```{r, label = 'randIntrandSlopesprof'}
mean_prof_score.random_slope_and_intercepts_model <- lmer(is_will_construction ~ mean_prof_score + (1 + mean_prof_score | anon_id), data = future_tokens_data)

summary(mean_prof_score.random_slope_and_intercepts_model)

mean_prof_score.random_slope_and_intercepts_model.coefs = coef(mean_prof_score.random_slope_and_intercepts_model)$anon_id %>% 
  rename(Intercept = `(Intercept)`) %>% 
  rownames_to_column("anon_id")
```

```{r, label = 'randIntAndSlopesByLevel'}
mean_prof_score.random_slope_and_intercepts_model.coefs %>% 
  left_join(future_tokens_data %>% select(anon_id, level_id, native_language) %>% mutate(level_id = factor(level_id)), by = 'anon_id') %>%
  distinct() %>%
  ggplot()+
  geom_point(aes(Intercept, mean_prof_score, color = level_id))+
  facet_wrap(~level_id)+
  labs(title = 'Intercept and Coefficient of Mean Proficency Score by Level')
```
```{r, label = 'randIntRandSlopeLevelBoxplotOfInts'}
mean_prof_score.random_slope_and_intercepts_model.coefs %>% 
  left_join(future_tokens_data %>% select(anon_id, level_id) %>% mutate(level_id = factor(level_id)), by = 'anon_id') %>% 
  distinct() %>% 
  ggplot(aes(level_id, Intercept))+
  geom_boxplot()+
  labs(x = 'Level', title = 'Intercepts by Level ID')

```
```{r, label = 'randIntRandSlopeLevelBoxplotOfSlopes'}
mean_prof_score.random_slope_and_intercepts_model.coefs %>% 
  left_join(future_tokens_data %>% select(anon_id, level_id) %>% mutate(level_id = factor(level_id)), by = 'anon_id') %>% 
  distinct() %>% 
  ggplot(aes(level_id, mean_prof_score))+
  geom_boxplot()+
  labs(x = 'Level', title = 'Slopes by Level ID', y = 'Slope')

```
```{r, label = 'countInLevel'}
mean_prof_score.random_slope_and_intercepts_model.coefs %>% 
  left_join(future_tokens_data %>% select(anon_id, level_id) %>% mutate(level_id = factor(level_id)), by = 'anon_id') %>% 
  distinct() %>% count(level_id)
```


#### ANOVAs for Random Intercepts and Random Slopes by Level

```{r}
anova(mean_prof_score.random_slope_and_intercepts_model)
```



```{r, label = 'anovaInts'}

#ANOVA for Intercepts
print('ANOVA for Intercepts by Level')
summary(aov(Intercept ~ level_id, data = mean_prof_score.random_slope_and_intercepts_model.coefs %>% 
  left_join(future_tokens_data %>% select(anon_id, level_id, native_language) %>% mutate(level_id = factor(level_id)), by = 'anon_id') %>%
  distinct()))
```


```{r, label = 'anovaSlope'}
print('ANOVA for Slopes by Level')
summary(aov(mean_prof_score ~ level_id, data = mean_prof_score.random_slope_and_intercepts_model.coefs %>% 
  left_join(future_tokens_data %>% select(anon_id, level_id, native_language) %>% mutate(level_id = factor(level_id)), by = 'anon_id') %>%
  distinct()))


```
#### Without Level 2

```{r, label = 'randIntrandSlopeswoLv2'}
mean_prof_score.random_slope_and_intercepts_model.woL2 <- lmer(is_will_construction ~ mean_prof_score + (1 + mean_prof_score | anon_id), data = future_tokens_data %>% filter(level_id != 2))

summary(mean_prof_score.random_slope_and_intercepts_model.woL2)

mean_prof_score.random_slope_and_intercepts_modelWOL2.coefs = coef(mean_prof_score.random_slope_and_intercepts_model.woL2)$anon_id %>% 
  rename(Intercept = `(Intercept)`) %>% 
  rownames_to_column("anon_id")
```

```{r}
anova(mean_prof_score.random_slope_and_intercepts_model.woL2)
```



```{r, label = 'anovaIntswoLv2'}

#ANOVA for Intercepts
print('ANOVA for Intercepts by Level')
summary(aov(Intercept ~ level_id, data = mean_prof_score.random_slope_and_intercepts_model.coefs %>% 
  left_join(future_tokens_data %>% select(anon_id, level_id, native_language) %>% mutate(level_id = factor(level_id)), by = 'anon_id') %>%
  distinct()))
```

```{r, label = 'anovaSlopewoLv2'}
print('ANOVA for Slopes by Level')
summary(aov(mean_prof_score ~ level_id, data = mean_prof_score.random_slope_and_intercepts_model.coefs %>% 
  left_join(future_tokens_data %>% select(anon_id, level_id, native_language) %>% mutate(level_id = factor(level_id)), by = 'anon_id') %>%
  distinct()))
```


### Predict 'will' with Level using Random Intercepts and Random Slopes

```{r, label = 'randIntSloLevelWoLv2'}
level_id.intercepts_and_slope_model <- lmer(is_will_construction ~ level_id + (1 + level_id | anon_id), data = future_tokens_data   %>% mutate(level_id = level_id-2))

summary(level_id.intercepts_and_slope_model)

level_id.model_coefs = coef(level_id.intercepts_and_slope_model)$anon_id %>% 
  rename(Intercept = `(Intercept)`) %>% 
  rownames_to_column("anon_id")
```


```{r, label = 'randIntSloLevelBoxplot'}
level_id.model_coefs %>% 
  left_join(student_info %>% select(anon_id, native_language), by = 'anon_id') %>% 
  left_join(future_tokens_data %>% select(anon_id, level_id), by = 'anon_id') %>% 
  ggplot()+
  geom_boxplot(aes(x = factor(level_id.y), y = Intercept)) + 
  labs(x = "ELI Level", y = "Random Speaker Intercept", title = "Effects of ELI Level on Speaker Intercept")
```

```{r, label = 'randIntSloLevelBoxplotSlope'}
level_id.model_coefs %>% 
  left_join(student_info %>% select(anon_id, native_language), by = 'anon_id') %>% 
  left_join(future_tokens_data %>% select(anon_id, level_id), by = 'anon_id') %>% 
  ggplot()+
  geom_boxplot(aes(x = factor(level_id.y), y = level_id.x)) + 
  labs(x = "ELI Level", y = "Random Speaker Slope", title = "Effects of ELI Level on Speaker Slope")
```



```{r}
anova(level_id.intercepts_and_slope_model)
```

#### Without Level 2

```{r, label = 'randIntSloLevel'}
level_id.intercepts_and_slope_model.woLevel2 <- lmer(is_will_construction ~ level_id + (1 + level_id | anon_id), data = future_tokens_data  %>%  filter(level_id !=2) %>% mutate(level_id = level_id-2))

summary(level_id.intercepts_and_slope_model.woLevel2)

level_id.model_coefs.woLevel2 = coef(level_id.intercepts_and_slope_model.woLevel2)$anon_id %>% 
  rename(Intercept = `(Intercept)`) %>% 
  rownames_to_column("anon_id")
```
```{r}
anova(level_id.intercepts_and_slope_model.woLevel2)
```

## Longitudinal Analysis

```{r, label = 'longAn'}
#score Data
score_data = as_tibble(read.csv(url('https://github.com/ELI-Data-Mining-Group/PELIC-dataset/raw/master/corpus_files/test_scores.csv')))

#Find students who took the test more than once
n_occur <- data.frame(table(score_data$anon_id))
multiple_ids = n_occur[n_occur$Freq > 1,]$Var1

longit_score_data = score_data %>% 
  filter(trimws(anon_id) %in% multiple_ids)
  
```

```{r, label = 'longAnProf'}

future_tokens_data %>% 
  inner_join(longit_score_data, by = c('anon_id','semester')) %>% 
  distinct() %>% 
  mutate(scale_lct = scale(LCT_Score.y)) %>% 
  mutate(scale_mtelp = scale(MTELP_Conv_Score.y)) %>% 
  mutate(scale_wr = scale(Writing_Sample.y)) %>% 
  mutate(mean_prof_score = rowMeans(select(.,scale_lct, scale_mtelp, scale_wr), na.rm = T)) %>% 
  select(anon_id, semester, is_will_construction, is_goingTo_construction, mean_prof_score) %>% 
  group_by(anon_id, semester) %>% 
  summarise_all(mean) %>% 
  filter(n()>1) %>% 
  separate(semester,c('year',NA)) %>% 
  ggplot(aes(year, mean_prof_score, color = anon_id, group = anon_id)) +
  geom_point() + geom_line()+
  labs(title = 'Longitudinal Proficiency Increase', x = 'Year',y = 'Mean Proficiency Score (scaled)', color = 'Student (ID)')
  
```

```{r, label = 'longAnWill'}
future_tokens_data %>% 
  inner_join(longit_score_data, by = c('anon_id','semester')) %>% 
  distinct() %>% 
  mutate(scale_lct = scale(LCT_Score.y)) %>% 
  mutate(scale_mtelp = scale(MTELP_Conv_Score.y)) %>% 
  mutate(scale_wr = scale(Writing_Sample.y)) %>% 
  mutate(mean_prof_score = rowMeans(select(.,scale_lct, scale_mtelp, scale_wr), na.rm = T)) %>% 
  select(anon_id, semester, is_will_construction, is_goingTo_construction, mean_prof_score) %>% 
  group_by(anon_id, semester) %>% 
  summarise_all(mean) %>% 
  filter(n()>1) %>% 
  separate(semester,c('year',NA)) %>% 
  ggplot(aes(year, is_will_construction, color = anon_id, group = anon_id)) +
  geom_point() + geom_line()+
  labs(title = 'Proportion of will Construction Utilized', x = 'Year',y = 'Proportion of future tokens with will constr.', color = 'Student (ID)')
  
  

```

