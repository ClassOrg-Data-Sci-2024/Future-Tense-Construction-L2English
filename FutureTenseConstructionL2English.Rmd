---
title: "FutureTenseConstructionL2English"
author: "Daniel Crawford"
date: "04/02/2024"
output: 
  github_document: 
    toc: TRUE
---
EXISTING

# Correlation of Future Tense Construction Preference with Proficiency Scores for English L2 Learners

```{r}
#Import Packages
library(tidyverse)
```

### Load Data
Raw data can be found [here](https://github.com/ELI-Data-Mining-Group/PELIC-dataset/raw/master/PELIC_compiled.csv).

```{r}
#Read in Data from PELIC: 
PELIC_compiled = as_tibble(read.csv(url("https://github.com/ELI-Data-Mining-Group/PELIC-dataset/raw/master/PELIC_compiled.csv"), fileEncoding = "ISO-8859-1"))
nrow(PELIC_compiled)
ncol(PELIC_compiled)
head(PELIC_compiled)
```

The data contains about 46,000 entries, and a vast about of background data on the students. Of note will be L1, level, and profiency. 

```{r}
#Need to get demographic and proficiency scores for each student

student_info = as_tibble(read.csv(url("https://github.com/ELI-Data-Mining-Group/PELIC-dataset/raw/master/corpus_files/student_information.csv")))
nrow(student_info)
ncol(student_info)
head(student_info)
  
```

The student information for the 1,313 students expands on the demographics, and will be joined in later to the data. 

### Set the functions




```{r}

tlp_text_to_df = function(x){
  
  text_df = x %>% 
    str_remove("^\\[\\(") %>% 
    str_remove("\\)\\]$") %>% 
    str_split_1("\\), \\(") %>% 
    str_remove_all("'") %>% 
    as.tibble() %>% 
    separate(value, into = c("token","lemma","POS"), sep = ',')
  
  return(text_df)
  
}

count_going_to_then_verb = function(text_df){
  going_idx = which(text_df$token == "going")
  idxs = sort(unique(c(going_idx,going_idx+1,going_idx+2,going_idx + 3)))
  
  ctr = 0
  for (idx in going_idx){
      if (trimws(text_df$POS[idx+1]) == "TO"){
        if (startsWith(trimws(text_df$POS[idx+2]), 'V') || startsWith(trimws(text_df$POS[idx+3]), 'V')){
          ctr = ctr + 1
        }
    }
  }
  return(ctr)

}

count_will_MD = function(text_df){
  
  will_idx = which(text_df$token == "will")
  
  ctr = 0
  for (idx in will_idx){
      if (trimws(text_df$POS[idx]) == "MD"){
        ctr = ctr + 1
    }
  }
  return(ctr)

}


text_to_count_GOING = function(str){
  text_df = tlp_text_to_df(str)
  count = count_going_to_then_verb(text_df)
  return(count)
}

text_count_will_MD = function(str){
  text_df = tlp_text_to_df(str)
  count = count_will_MD(text_df)
  return(count)
}
  
```








```{r}

TEST_tlp_text_to_df = function(x){
  
  text_df = x %>% 
    str_remove("^\\[\\(") %>% 
    str_remove("\\)\\]$") %>% 
    str_split_1("\\), \\(") %>% 
    str_remove_all("'") %>% 
    as.tibble() %>% 
    filter(!startsWith(value, ",,")) %>% 
    separate(value, into = c('token','lemma','POS'), sep = ',')
  
  return(text_df)
  
}



tokenized_df = PELIC_compiled %>% 
  slice(1:10) %>% 
  mutate(tokenized_nested_df = map(tok_lem_POS, TEST_tlp_text_to_df)) %>% 
  select(answer_id, tokenized_nested_df) %>% 
  unnest(tokenized_nested_df)


tokenized_df
```







### Format the Data 
```{r}
PELIC_compiled %>% 
  #Optional Slicer - can have long run time if too many rows are used
  slice(28:32) %>% 
  #Get just the id and the text - this is preferable and equivalent to having to concat all the text form a user
  select(anon_id,tok_lem_POS) %>% 
  #Count number of occurrences of will construction
  mutate(will_ct = map(tok_lem_POS, count_will_MD(.))) %>% 
  #Count number of occurrences going to constructions
  mutate(goingto_ct = map(tok_lem_POS,text_to_count_GOING(.))) %>% 
  #Group By individual
  group_by(anon_id) %>% 
  #sum the construction counts
  summarise_at(c("will_ct", "goingto_ct"),sum) %>% 
  left_join(
    (
      student_info %>% 
        select(anon_id,gender,gender,birth_year,native_language,yrs_in_english_environment,yrs_of_english_learning)
    ),
      by = 'anon_id'
    
  ) 
  #write.csv("working_data.csv")
        
```


My strategy for importing the data is to find construction in question in the text, and then to join in the student demographic information. The `working_data.csv` is the current end product of my data. 






# Session Info
```{r}
sessionInfo()
```

